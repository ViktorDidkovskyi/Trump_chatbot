{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# About\n",
    "\n",
    "The overall goal of this tutorial is to create a language learning Trump ChatBot where you can practice simple conversations in a language you care about. To reach the goal we\n",
    "will finetune small DialoGPT https://huggingface.co/transformers/model_doc/dialogpt.html. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "The main bottleneck is the dataset. It would be great to have some kind of Trump dialog. In the first iteration I propose to use data from the election debate:\n",
    "* Debate with Hillary Clinton ( first and second): https://github.com/wimlds/election-data-hackathon\n",
    "* First debate with John Biden: https://www.kaggle.com/headsortails/us-election-2020-presidential-debates?select=us_election_2020_1st_presidential_debate.csv\n",
    "* Second debate with John Biden: https://www.kaggle.com/headsortails/us-election-2020-presidential-debates?select=us_election_2020_2nd_presidential_debate.csv\n",
    "* Trump town hall: https://www.kaggle.com/headsortails/us-election-2020-presidential-debates?select=us_election_2020_trump_town_hall.csv\n",
    "\n",
    "This data is not perfect enough and has a lot of weakness. For example, a long answer. \n",
    "\n",
    "How can we increase the amount of data? - for the next iteration we can use data from Wikipedia, tweets, and Trump's speech + model: https://github.com/patil-suraj/question_generation to generate the question for the text. Small experiment how it is working please find in Additional_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-3b45d00f9755>:5: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_colwidth', -1) \n",
    "pd.set_option('display.max_rows', 150)\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /private/var/folders/5x/9tykzp5x2hs34ly3kyq8pp8m0000gn/T/pip-req-build-los__npd\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: sacremoses in /Users/viktor/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.2.0.dev0) (0.0.43)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/viktor/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.2.0.dev0) (4.50.2)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /Users/viktor/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.2.0.dev0) (0.9.4)\n",
      "Requirement already satisfied: numpy in /Users/viktor/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.2.0.dev0) (1.19.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/viktor/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.2.0.dev0) (2020.10.15)\n",
      "Requirement already satisfied: packaging in /Users/viktor/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.2.0.dev0) (20.4)\n",
      "Requirement already satisfied: requests in /Users/viktor/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.2.0.dev0) (2.24.0)\n",
      "Requirement already satisfied: filelock in /Users/viktor/opt/anaconda3/lib/python3.8/site-packages (from transformers==4.2.0.dev0) (3.0.12)\n",
      "Requirement already satisfied: click in /Users/viktor/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==4.2.0.dev0) (7.1.2)\n",
      "Requirement already satisfied: six in /Users/viktor/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==4.2.0.dev0) (1.15.0)\n",
      "Requirement already satisfied: joblib in /Users/viktor/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==4.2.0.dev0) (0.17.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/viktor/opt/anaconda3/lib/python3.8/site-packages (from packaging->transformers==4.2.0.dev0) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/viktor/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.2.0.dev0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/viktor/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.2.0.dev0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/viktor/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.2.0.dev0) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/viktor/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.2.0.dev0) (3.0.4)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.2.0.dev0-py3-none-any.whl size=1527263 sha256=23c05fe175b4d10a17da913eaa6b0fe44e982ca1e5db0c41f54e6ac89646c144\n",
      "  Stored in directory: /private/var/folders/5x/9tykzp5x2hs34ly3kyq8pp8m0000gn/T/pip-ephem-wheel-cache-9p0xr3d4/wheels/42/68/45/c63edff61c292f2dfd4df4ef6522dcbecc603e7af82813c1d7\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.1.1\n",
      "    Uninstalling transformers-4.1.1:\n",
      "      Successfully uninstalled transformers-4.1.1\n",
      "Successfully installed transformers-4.2.0.dev0\n"
     ]
    }
   ],
   "source": [
    "! pip install mosestokenizer\n",
    "! pip install unidecode\n",
    "! pip install blingfire\n",
    "! pip install torch\n",
    "! pip install git+https://github.com/huggingface/transformers\n",
    "! pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of model without finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: Good evening!\n",
      "Trump: Good morning!\n",
      ">> User: Are you planning to start a nuclear war with North Korea?\n",
      "Trump: Good evening!\n",
      ">> User: Are you planning to continue the economic war with China?\n",
      "Trump: Greetings! Good evening! Good afternoon!\n",
      ">> User: Have you been supported by Russia during the election?\n",
      "Trump: I am an American citizen and I support this decision.\n",
      ">> User: How do you feel about the Black Life Matters?\n",
      "Trump: They do.\n",
      ">> User: How are you planning to overcome Covid-19?\n",
      "Trump: The war will be won by a large majority of people.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\") #medium\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "\n",
    "\n",
    "# Let's chat for 6 lines\n",
    "for step in range(6):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User: \") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=300,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=100, \n",
    "        top_p=0.7,\n",
    "        temperature = 0.8\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"Trump: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The answers are not good enough. Let's fine-tune the model and repeat the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\n",
    "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n",
    "using a masked language modeling (MLM) loss.\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import (\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from blingfire import *\n",
    "from mosestokenizer import *\n",
    "from unidecode import unidecode\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "# Configs\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define to configuration variables so we don't have a bunch of magic numbers and strings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Args to allow for easy convertion of python script to notebook\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.output_dir = 'output'\n",
    "        self.model_type = 'gpt2'\n",
    "        self.model_name_or_path = 'microsoft/DialoGPT-small'\n",
    "        self.config_name = 'microsoft/DialoGPT-small'\n",
    "        self.tokenizer_name = 'microsoft/DialoGPT-small'\n",
    "        self.cache_dir = 'cached'\n",
    "        self.block_size = 512\n",
    "        self.do_train = True\n",
    "        self.do_eval = True\n",
    "        self.evaluate_during_training = True #False\n",
    "        self.per_gpu_train_batch_size = 4 # 4\n",
    "        self.per_gpu_eval_batch_size = 4 # 4\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 3\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.logging_steps = 1000\n",
    "        self.save_steps = 3500\n",
    "        self.save_total_limit = None\n",
    "        self.eval_all_checkpoints = False\n",
    "        self.no_cuda = False\n",
    "        self.overwrite_output_dir = True\n",
    "        self.overwrite_cache = True\n",
    "        self.should_continue = False\n",
    "        self.seed = None\n",
    "        self.local_rank = -1\n",
    "        self.fp16 = False\n",
    "        self.fp16_opt_level = 'O1'\n",
    "        self.path_to_data = \"/Users/viktor/Documents/GitHub/Project/Trump_chatbot/data/debate\"\n",
    "        self.n_tokens = 200 # maximum N of tokens that we will leave in dataset\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's read the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hillary_trump_debat = pd.read_csv(args.path_to_data + \"/hillary/debate.csv\")\n",
    "\n",
    "biden_trump_debat_1 = pd.read_csv(args.path_to_data + \"/biden/us_election_2020_1st_presidential_debate.csv\")\n",
    "biden_trump_debat_2 = pd.read_csv(args.path_to_data + \"/biden/us_election_2020_2nd_presidential_debate.csv\")\n",
    "\n",
    "trump_town_hall = pd.read_csv(args.path_to_data + \"/biden/us_election_2020_trump_town_hall.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>minute</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kristen Welker</td>\n",
       "      <td>00:18</td>\n",
       "      <td>Good evening, everyone. Good evening. Thank you so much for being here. It is such an honor for me to moderate this debate tonight, the final debate. I want to welcome the first family and the first lady. We’re so glad and thankful that you are feeling better. I want to welcome the Biden family, Dr. Jill Biden. Thank you all for being here tonight. We are so excited. We’re looking forward to a really robust discussion. And the only thing I would reiterate are the CPD guidelines that when the candidates are talking, please hold any applause or any other reactions. Except of course, when they walk out, make sure you cheer and loud and applause so that everyone can hear you. Thank you for having me. This is really the honor of a lifetime. I am going to sit down and just get organized and get settled and the show will start very soon. Thank you for being here. (silence). Good evening from Belmont University in Nashville, Tennessee. I’m Kristen Welker of NBC News. And I welcome you to the final 2020 presidential debate between President Donald J. Trump and former vice president Joe Biden. Tonight’s debate is sponsored by the Commission on Presidential Debates. It is conducted under health and safety protocols designed by the Commission’s health security advisor. The audience here in the hall has promised to remain silent. No cheers, boos, or other interruptions, except right now, as we welcome to the stage, former vice president Joe Biden and President Donald J. Trump.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>07:37</td>\n",
       "      <td>How are you doing? How are you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kristen Welker</td>\n",
       "      <td>07:58</td>\n",
       "      <td>And I do want to say a very good evening to both of you. This debate will cover six major topics. At the beginning of each section, each candidate will have two minutes, uninterrupted, to answer my first question. The Debate Commission will then turn on their microphone only when it is their turn to answer. And the Commission will turn it off exactly when the two minutes have expired. After that, both microphones will remain on. But on behalf of the voters, I’m going to ask you to please speak one at a time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kristen Welker</td>\n",
       "      <td>08:27</td>\n",
       "      <td>The goal is for you to hear each other and for the American people to hear every word of what you both have to say. And so with that, if you’re ready, let’s start. And we will begin with the fight against the coronavirus. President Trump, the first question is for you. The country is heading into a dangerous new phase. More than 40,000 Americans are in the hospital tonight with COVID, including record numbers here in Tennessee. And since the two of you last shared a stage, 16,000 Americans have died from COVID. So please be specific. How would you lead the country during this next stage of the coronavirus crisis? Two minutes, uninterrupted.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kristen Welker</td>\n",
       "      <td>09:03</td>\n",
       "      <td>… during this next stage of the coronavirus crisis. Two minutes uninterrupted.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          speaker minute  \\\n",
       "0  Kristen Welker  00:18   \n",
       "1  Donald Trump    07:37   \n",
       "2  Kristen Welker  07:58   \n",
       "3  Kristen Welker  08:27   \n",
       "4  Kristen Welker  09:03   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               text  \n",
       "0  Good evening, everyone. Good evening. Thank you so much for being here. It is such an honor for me to moderate this debate tonight, the final debate. I want to welcome the first family and the first lady. We’re so glad and thankful that you are feeling better. I want to welcome the Biden family, Dr. Jill Biden. Thank you all for being here tonight. We are so excited. We’re looking forward to a really robust discussion. And the only thing I would reiterate are the CPD guidelines that when the candidates are talking, please hold any applause or any other reactions. Except of course, when they walk out, make sure you cheer and loud and applause so that everyone can hear you. Thank you for having me. This is really the honor of a lifetime. I am going to sit down and just get organized and get settled and the show will start very soon. Thank you for being here. (silence). Good evening from Belmont University in Nashville, Tennessee. I’m Kristen Welker of NBC News. And I welcome you to the final 2020 presidential debate between President Donald J. Trump and former vice president Joe Biden. Tonight’s debate is sponsored by the Commission on Presidential Debates. It is conducted under health and safety protocols designed by the Commission’s health security advisor. The audience here in the hall has promised to remain silent. No cheers, boos, or other interruptions, except right now, as we welcome to the stage, former vice president Joe Biden and President Donald J. Trump.  \n",
       "1  How are you doing? How are you?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "2  And I do want to say a very good evening to both of you. This debate will cover six major topics. At the beginning of each section, each candidate will have two minutes, uninterrupted, to answer my first question. The Debate Commission will then turn on their microphone only when it is their turn to answer. And the Commission will turn it off exactly when the two minutes have expired. After that, both microphones will remain on. But on behalf of the voters, I’m going to ask you to please speak one at a time.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "3  The goal is for you to hear each other and for the American people to hear every word of what you both have to say. And so with that, if you’re ready, let’s start. And we will begin with the fight against the coronavirus. President Trump, the first question is for you. The country is heading into a dangerous new phase. More than 40,000 Americans are in the hospital tonight with COVID, including record numbers here in Tennessee. And since the two of you last shared a stage, 16,000 Americans have died from COVID. So please be specific. How would you lead the country during this next stage of the coronavirus crisis? Two minutes, uninterrupted.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "4  … during this next stage of the coronavirus crisis. Two minutes uninterrupted.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biden_trump_debat_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden_trump_debat_2 = biden_trump_debat_2[biden_trump_debat_2['text'] != '[Crosstalk 00:24:31].']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hillary_trump_debat = hillary_trump_debat[~hillary_trump_debat.Speaker.isin(['CANDIDATES','Audience'])]\n",
    "hillary_trump_debat = hillary_trump_debat[hillary_trump_debat.Date !='2016-10-04']\n",
    "hillary_trump_debat.columns = [col.lower() for col in hillary_trump_debat.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hillary_trump_debat1 = hillary_trump_debat[hillary_trump_debat.date =='2016-09-26'].reset_index(drop=True)\n",
    "hillary_trump_debat2 = hillary_trump_debat[hillary_trump_debat.date =='2016-10-09'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataframe2response_context(df_in: pd.DataFrame, column_w_speaker: str='speaker',\n",
    "                                       column_w_text: str='text', trump_name_in_df: str='President Donald J. Trump',\n",
    "                                       context_len: int=5):\n",
    "    \n",
    "    df_in = df_in.reset_index(drop=True)\n",
    "    trump_told = df_in[df_in[column_w_speaker] == trump_name_in_df]\n",
    "\n",
    "    contexted = []\n",
    "\n",
    "\n",
    "    for i in list(trump_told.index):\n",
    "\n",
    "        # we substract 1, so row will contain trump_responce and 7 previous responces\n",
    "        prev = max(i - 1 - context_len, 0) \n",
    "        row = [df_in.loc[j, column_w_text] for j in range(i, prev, -1)]\n",
    "\n",
    "        contexted.append(row)  \n",
    "\n",
    "    columns = ['Trump_said', 'context'] \n",
    "    columns = columns + ['context/'+str(i) for i in range(context_len-1)]   \n",
    "\n",
    "    df_out = pd.DataFrame.from_records(contexted, columns=columns)\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "\n",
    "\n",
    "def text_to_words(s: str):\n",
    "    \"\"\"convert text to word\"\"\"\n",
    "\n",
    "    # get the UTF-8 bytes\n",
    "    s_bytes = s.encode(\"utf-8\")\n",
    "\n",
    "    # allocate the output buffer\n",
    "    o_bytes = create_string_buffer(len(s_bytes) * 3)\n",
    "    o_bytes_count = len(o_bytes)\n",
    "\n",
    "    # identify paragraphs\n",
    "    o_len = blingfire.TextToWords(c_char_p(s_bytes), c_int(len(s_bytes)), byref(o_bytes), c_int(o_bytes_count))\n",
    "\n",
    "    # check if no error has happened\n",
    "    if -1 == o_len or o_len > o_bytes_count:\n",
    "        return ''\n",
    "\n",
    "    # compute the unicode string from the UTF-8 bytes\n",
    "    return o_bytes.value.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert each dataset into format: trump speach + context\n",
    "Also, each input text we will normalized and leave maximum args.n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stdbuf was not found; communication with perl may hang due to stdio buffering.\n"
     ]
    }
   ],
   "source": [
    "list_of_df = []\n",
    "\n",
    "detokenize = MosesDetokenizer('en')\n",
    "for df_to_convert, trump_name in zip([hillary_trump_debat1, hillary_trump_debat2, biden_trump_debat_1, biden_trump_debat_2, trump_town_hall],\n",
    "                                     ['Trump', 'Trump', 'President Donald J. Trump', 'Donald Trump', 'President Trump']):\n",
    "    ## normalized the text\n",
    "    df_to_convert['norm_text'] = df_to_convert.text.replace('-', ' ').apply(lambda x: text_to_words(unidecode(str(x))).replace('&', 'and'))\n",
    "    ## leave only n tokens maximum\n",
    "    df_to_convert['norm_text'] = df_to_convert['norm_text'].apply(lambda x: detokenize(x.split(' ')[:args.n_tokens]))\n",
    "    \n",
    "    df_converted = convert_dataframe2response_context(df_to_convert, column_w_speaker='speaker', column_w_text='norm_text',\n",
    "                                  trump_name_in_df=trump_name, context_len=5)\n",
    "\n",
    "    list_of_df.append(df_converted)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all dataset in one dataframe and remove line without context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trump_said</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "      <th>context/2</th>\n",
       "      <th>context/3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you, Lester. Our jobs are fleeing the country. They're going to Mexico. They're going to many other countries. You look at what China is doing to our country in terms of making our product. They're devaluing their currency, and there's nobody in our government to fight them. And we have a very good fight. And we have a winning fight. Because they're using our country as a piggy bank to rebuild China, and many other countries are doing the same thing. So we're losing our good jobs, so many of them. When you look at what's happening in Mexico, a friend of mine who builds plants said it's the eighth wonder of the world. They're building some of the biggest plants anywhere in the world, some of the most sophisticated, some of the best plants. With the United States, as he said, not so much. So Ford is leaving. You see that, their small car division leaving. Thousands of jobs leaving Michigan, leaving Ohio</td>\n",
       "      <td>Secretary Clinton, thank you. Mr. Trump, the same question to you. It's about putting money - - more money into the pockets of American workers. You have up to two minutes.</td>\n",
       "      <td>I also want to see more companies do profit - sharing. If you help create the profits, you should be able to share in them, not just the executives at the top. And I want us to do more to support people who are struggling to balance family and work. I've heard from so many of you about the difficult choices you face and the stresses that you're under. So let's have paid family leave, earned sick days. Let's be sure we have affordable child care and debt - free college. How are we going to do it? We're going to do it by having the wealthy pay their fair share and close the corporate loopholes. Finally, we tonight are on the stage together, Donald Trump and I. Donald, it's good to be with you. We're going to have a debate where we are talking about the important issues facing our country. You have to judge us, who can shoulder the immense, awesome responsibilities of the presidency, who can put into action the</td>\n",
       "      <td>Well, thank you, Lester, and thanks to Hofstra for hosting us. The central question in this election is really what kind of country we want to be and what kind of future we'll build together. Today is my granddaughter's second birthday, so I think about this a lot. First, we have to build an economy that works for everyone, not just those at the top. That means we need new jobs, good jobs, with rising incomes. I want us to invest in you. I want us to invest in your future. That means jobs in infrastructure, in advanced manufacturing, innovation and technology, clean, renewable energy, and small business, because most of the new jobs will come from small business. We also have to make the economy fairer. That starts with raising the national minimum wage and also guarantee, finally, equal pay for women's work.</td>\n",
       "      <td>Well, I do n't expect us to cover all the issues of this campaign tonight, but I remind everyone, there are two more presidential debates scheduled. We are going to focus on many of the issues that voters tell us are most important, and we're going to press for specifics. I am honored to have this role, but this evening belongs to the candidates and, just as important, to the American people. Candidates, we look forward to hearing you articulate your policies and your positions, as well as your visions and your values. So, let's begin. We're calling this opening segment \"Achieving Prosperity.\" And central to that is jobs. There are two economic realities in America today. There's been a record six straight years of job growth, and new census numbers show incomes have increased at a record rate after years of stagnation. However, income inequality remains significant, and nearly half of Americans are living paycheck to paycheck. Beginning with you, Secretary Clinton, why are you a better choice</td>\n",
       "      <td>Good luck to you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We can not let it happen. Under my plan, I'll be reducing taxes tremendously, from 35 percent to 15 percent for companies, small and big businesses. That's going to be a job creator like we have n't seen since Ronald Reagan. It's going to be a beautiful thing to watch. Companies will come. They will build. They will expand. New companies will start. And I look very, very much forward to doing it. We have to renegotiate our trade deals, and we have to stop these countries from stealing our companies and our jobs.</td>\n",
       "      <td>Thank you, Lester. Our jobs are fleeing the country. They're going to Mexico. They're going to many other countries. You look at what China is doing to our country in terms of making our product. They're devaluing their currency, and there's nobody in our government to fight them. And we have a very good fight. And we have a winning fight. Because they're using our country as a piggy bank to rebuild China, and many other countries are doing the same thing. So we're losing our good jobs, so many of them. When you look at what's happening in Mexico, a friend of mine who builds plants said it's the eighth wonder of the world. They're building some of the biggest plants anywhere in the world, some of the most sophisticated, some of the best plants. With the United States, as he said, not so much. So Ford is leaving. You see that, their small car division leaving. Thousands of jobs leaving Michigan, leaving Ohio</td>\n",
       "      <td>Secretary Clinton, thank you. Mr. Trump, the same question to you. It's about putting money - - more money into the pockets of American workers. You have up to two minutes.</td>\n",
       "      <td>I also want to see more companies do profit - sharing. If you help create the profits, you should be able to share in them, not just the executives at the top. And I want us to do more to support people who are struggling to balance family and work. I've heard from so many of you about the difficult choices you face and the stresses that you're under. So let's have paid family leave, earned sick days. Let's be sure we have affordable child care and debt - free college. How are we going to do it? We're going to do it by having the wealthy pay their fair share and close the corporate loopholes. Finally, we tonight are on the stage together, Donald Trump and I. Donald, it's good to be with you. We're going to have a debate where we are talking about the important issues facing our country. You have to judge us, who can shoulder the immense, awesome responsibilities of the presidency, who can put into action the</td>\n",
       "      <td>Well, thank you, Lester, and thanks to Hofstra for hosting us. The central question in this election is really what kind of country we want to be and what kind of future we'll build together. Today is my granddaughter's second birthday, so I think about this a lot. First, we have to build an economy that works for everyone, not just those at the top. That means we need new jobs, good jobs, with rising incomes. I want us to invest in you. I want us to invest in your future. That means jobs in infrastructure, in advanced manufacturing, innovation and technology, clean, renewable energy, and small business, because most of the new jobs will come from small business. We also have to make the economy fairer. That starts with raising the national minimum wage and also guarantee, finally, equal pay for women's work.</td>\n",
       "      <td>Well, I do n't expect us to cover all the issues of this campaign tonight, but I remind everyone, there are two more presidential debates scheduled. We are going to focus on many of the issues that voters tell us are most important, and we're going to press for specifics. I am honored to have this role, but this evening belongs to the candidates and, just as important, to the American people. Candidates, we look forward to hearing you articulate your policies and your positions, as well as your visions and your values. So, let's begin. We're calling this opening segment \"Achieving Prosperity.\" And central to that is jobs. There are two economic realities in America today. There's been a record six straight years of job growth, and new census numbers show incomes have increased at a record rate after years of stagnation. However, income inequality remains significant, and nearly half of Americans are living paycheck to paycheck. Beginning with you, Secretary Clinton, why are you a better choice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Well, for one thing - - and before we start on that - - my father gave me a very small loan in 1975, and I built it into a company that's worth many, many billions of dollars, with some of the greatest assets in the world, and I say that only because that's the kind of thinking that our country needs. Our country's in deep trouble. We do n't know what we're doing when it comes to devaluations and all of these countries all over the world, especially China. They're the best, the best ever at it. What they're doing to us is a very, very sad thing. So we have to do that. We have to renegotiate our trade deals. And, Lester, they're taking our jobs, they're giving incentives, they're doing things that, frankly, we do n't do. Let me give you the example of Mexico. They have a VAT tax. We're on a different system. When we sell into Mexico, there</td>\n",
       "      <td>Let me follow up with Mr. Trump, if you can. You've talked about creating 25 million jobs, and you've promised to bring back millions of jobs for Americans. How are you going to bring back the industries that have left this country for cheaper labor overseas? How, specifically, are you going to tell American manufacturers that you have to come back?</td>\n",
       "      <td>Well, I think that trade is an important issue. Of course, we are 5 percent of the world's population; we have to trade with the other 95 percent. And we need to have smart, fair trade deals. We also, though, need to have a tax system that rewards work and not just financial transactions. And the kind of plan that Donald has put forth would be trickle - down economics all over again. In fact, it would be the most extreme version, the biggest tax cuts for the top percent of the people in this country than we've ever had. I call it trumped - up trickle - down, because that's exactly what it would be. That is not how we grow the economy. We just have a different view about what's best for growing the economy, how we make investments that will actually produce jobs and rising incomes. I think we come at it from somewhat different perspectives. I understand that. You know, Donald was very fortunate in his life, and that</td>\n",
       "      <td>Secretary Clinton, would you like to respond?</td>\n",
       "      <td>We can not let it happen. Under my plan, I'll be reducing taxes tremendously, from 35 percent to 15 percent for companies, small and big businesses. That's going to be a job creator like we have n't seen since Ronald Reagan. It's going to be a beautiful thing to watch. Companies will come. They will build. They will expand. New companies will start. And I look very, very much forward to doing it. We have to renegotiate our trade deals, and we have to stop these countries from stealing our companies and our jobs.</td>\n",
       "      <td>Thank you, Lester. Our jobs are fleeing the country. They're going to Mexico. They're going to many other countries. You look at what China is doing to our country in terms of making our product. They're devaluing their currency, and there's nobody in our government to fight them. And we have a very good fight. And we have a winning fight. Because they're using our country as a piggy bank to rebuild China, and many other countries are doing the same thing. So we're losing our good jobs, so many of them. When you look at what's happening in Mexico, a friend of mine who builds plants said it's the eighth wonder of the world. They're building some of the biggest plants anywhere in the world, some of the most sophisticated, some of the best plants. With the United States, as he said, not so much. So Ford is leaving. You see that, their small car division leaving. Thousands of jobs leaving Michigan, leaving Ohio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Secretary Clinton and others, politicians, should have been doing this for years, not right now, because of the fact that we've created a movement. They should have been doing this for years. What's happened to our jobs and our country and our economy generally is - - look, we owe $20 trillion. We can not do it any longer, Lester.</td>\n",
       "      <td>Let me interrupt just a moment, but...</td>\n",
       "      <td>Well, for one thing - - and before we start on that - - my father gave me a very small loan in 1975, and I built it into a company that's worth many, many billions of dollars, with some of the greatest assets in the world, and I say that only because that's the kind of thinking that our country needs. Our country's in deep trouble. We do n't know what we're doing when it comes to devaluations and all of these countries all over the world, especially China. They're the best, the best ever at it. What they're doing to us is a very, very sad thing. So we have to do that. We have to renegotiate our trade deals. And, Lester, they're taking our jobs, they're giving incentives, they're doing things that, frankly, we do n't do. Let me give you the example of Mexico. They have a VAT tax. We're on a different system. When we sell into Mexico, there</td>\n",
       "      <td>Let me follow up with Mr. Trump, if you can. You've talked about creating 25 million jobs, and you've promised to bring back millions of jobs for Americans. How are you going to bring back the industries that have left this country for cheaper labor overseas? How, specifically, are you going to tell American manufacturers that you have to come back?</td>\n",
       "      <td>Well, I think that trade is an important issue. Of course, we are 5 percent of the world's population; we have to trade with the other 95 percent. And we need to have smart, fair trade deals. We also, though, need to have a tax system that rewards work and not just financial transactions. And the kind of plan that Donald has put forth would be trickle - down economics all over again. In fact, it would be the most extreme version, the biggest tax cuts for the top percent of the people in this country than we've ever had. I call it trumped - up trickle - down, because that's exactly what it would be. That is not how we grow the economy. We just have a different view about what's best for growing the economy, how we make investments that will actually produce jobs and rising incomes. I think we come at it from somewhat different perspectives. I understand that. You know, Donald was very fortunate in his life, and that</td>\n",
       "      <td>Secretary Clinton, would you like to respond?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, the first thing you do is do n't let the jobs leave. The companies are leaving. I could name, I mean, there are thousands of them. They're leaving, and they're leaving in bigger numbers than ever. And what you do is you say, fine, you want to go to Mexico or some other country, good luck. We wish you a lot of luck. But if you think you're going to make your air conditioners or your cars or your cookies or whatever you make and bring them into our country without a tax, you're wrong. And once you say you're going to have to tax them coming in, and our politicians never do this, because they have special interests and the special interests want those companies to leave, because in many cases, they own the companies. So what I 'm saying is, we can stop them from leaving. We have to stop them from leaving. And that's a big, big factor.</td>\n",
       "      <td>Back to the question, though. How do you bring back - - specifically bring back jobs, American manufacturers? How do you make them bring the jobs back?</td>\n",
       "      <td>Secretary Clinton and others, politicians, should have been doing this for years, not right now, because of the fact that we've created a movement. They should have been doing this for years. What's happened to our jobs and our country and our economy generally is - - look, we owe $20 trillion. We can not do it any longer, Lester.</td>\n",
       "      <td>Let me interrupt just a moment, but...</td>\n",
       "      <td>Well, for one thing - - and before we start on that - - my father gave me a very small loan in 1975, and I built it into a company that's worth many, many billions of dollars, with some of the greatest assets in the world, and I say that only because that's the kind of thinking that our country needs. Our country's in deep trouble. We do n't know what we're doing when it comes to devaluations and all of these countries all over the world, especially China. They're the best, the best ever at it. What they're doing to us is a very, very sad thing. So we have to do that. We have to renegotiate our trade deals. And, Lester, they're taking our jobs, they're giving incentives, they're doing things that, frankly, we do n't do. Let me give you the example of Mexico. They have a VAT tax. We're on a different system. When we sell into Mexico, there</td>\n",
       "      <td>Let me follow up with Mr. Trump, if you can. You've talked about creating 25 million jobs, and you've promised to bring back millions of jobs for Americans. How are you going to bring back the industries that have left this country for cheaper labor overseas? How, specifically, are you going to tell American manufacturers that you have to come back?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Trump_said  \\\n",
       "0  Thank you, Lester. Our jobs are fleeing the country. They're going to Mexico. They're going to many other countries. You look at what China is doing to our country in terms of making our product. They're devaluing their currency, and there's nobody in our government to fight them. And we have a very good fight. And we have a winning fight. Because they're using our country as a piggy bank to rebuild China, and many other countries are doing the same thing. So we're losing our good jobs, so many of them. When you look at what's happening in Mexico, a friend of mine who builds plants said it's the eighth wonder of the world. They're building some of the biggest plants anywhere in the world, some of the most sophisticated, some of the best plants. With the United States, as he said, not so much. So Ford is leaving. You see that, their small car division leaving. Thousands of jobs leaving Michigan, leaving Ohio   \n",
       "1  We can not let it happen. Under my plan, I'll be reducing taxes tremendously, from 35 percent to 15 percent for companies, small and big businesses. That's going to be a job creator like we have n't seen since Ronald Reagan. It's going to be a beautiful thing to watch. Companies will come. They will build. They will expand. New companies will start. And I look very, very much forward to doing it. We have to renegotiate our trade deals, and we have to stop these countries from stealing our companies and our jobs.                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "2  Well, for one thing - - and before we start on that - - my father gave me a very small loan in 1975, and I built it into a company that's worth many, many billions of dollars, with some of the greatest assets in the world, and I say that only because that's the kind of thinking that our country needs. Our country's in deep trouble. We do n't know what we're doing when it comes to devaluations and all of these countries all over the world, especially China. They're the best, the best ever at it. What they're doing to us is a very, very sad thing. So we have to do that. We have to renegotiate our trade deals. And, Lester, they're taking our jobs, they're giving incentives, they're doing things that, frankly, we do n't do. Let me give you the example of Mexico. They have a VAT tax. We're on a different system. When we sell into Mexico, there                                                                         \n",
       "3  Secretary Clinton and others, politicians, should have been doing this for years, not right now, because of the fact that we've created a movement. They should have been doing this for years. What's happened to our jobs and our country and our economy generally is - - look, we owe $20 trillion. We can not do it any longer, Lester.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "4  Well, the first thing you do is do n't let the jobs leave. The companies are leaving. I could name, I mean, there are thousands of them. They're leaving, and they're leaving in bigger numbers than ever. And what you do is you say, fine, you want to go to Mexico or some other country, good luck. We wish you a lot of luck. But if you think you're going to make your air conditioners or your cars or your cookies or whatever you make and bring them into our country without a tax, you're wrong. And once you say you're going to have to tax them coming in, and our politicians never do this, because they have special interests and the special interests want those companies to leave, because in many cases, they own the companies. So what I 'm saying is, we can stop them from leaving. We have to stop them from leaving. And that's a big, big factor.                                                                          \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    context  \\\n",
       "0  Secretary Clinton, thank you. Mr. Trump, the same question to you. It's about putting money - - more money into the pockets of American workers. You have up to two minutes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "1  Thank you, Lester. Our jobs are fleeing the country. They're going to Mexico. They're going to many other countries. You look at what China is doing to our country in terms of making our product. They're devaluing their currency, and there's nobody in our government to fight them. And we have a very good fight. And we have a winning fight. Because they're using our country as a piggy bank to rebuild China, and many other countries are doing the same thing. So we're losing our good jobs, so many of them. When you look at what's happening in Mexico, a friend of mine who builds plants said it's the eighth wonder of the world. They're building some of the biggest plants anywhere in the world, some of the most sophisticated, some of the best plants. With the United States, as he said, not so much. So Ford is leaving. You see that, their small car division leaving. Thousands of jobs leaving Michigan, leaving Ohio   \n",
       "2  Let me follow up with Mr. Trump, if you can. You've talked about creating 25 million jobs, and you've promised to bring back millions of jobs for Americans. How are you going to bring back the industries that have left this country for cheaper labor overseas? How, specifically, are you going to tell American manufacturers that you have to come back?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "3  Let me interrupt just a moment, but...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "4  Back to the question, though. How do you bring back - - specifically bring back jobs, American manufacturers? How do you make them bring the jobs back?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          context/0  \\\n",
       "0  I also want to see more companies do profit - sharing. If you help create the profits, you should be able to share in them, not just the executives at the top. And I want us to do more to support people who are struggling to balance family and work. I've heard from so many of you about the difficult choices you face and the stresses that you're under. So let's have paid family leave, earned sick days. Let's be sure we have affordable child care and debt - free college. How are we going to do it? We're going to do it by having the wealthy pay their fair share and close the corporate loopholes. Finally, we tonight are on the stage together, Donald Trump and I. Donald, it's good to be with you. We're going to have a debate where we are talking about the important issues facing our country. You have to judge us, who can shoulder the immense, awesome responsibilities of the presidency, who can put into action the          \n",
       "1  Secretary Clinton, thank you. Mr. Trump, the same question to you. It's about putting money - - more money into the pockets of American workers. You have up to two minutes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "2  Well, I think that trade is an important issue. Of course, we are 5 percent of the world's population; we have to trade with the other 95 percent. And we need to have smart, fair trade deals. We also, though, need to have a tax system that rewards work and not just financial transactions. And the kind of plan that Donald has put forth would be trickle - down economics all over again. In fact, it would be the most extreme version, the biggest tax cuts for the top percent of the people in this country than we've ever had. I call it trumped - up trickle - down, because that's exactly what it would be. That is not how we grow the economy. We just have a different view about what's best for growing the economy, how we make investments that will actually produce jobs and rising incomes. I think we come at it from somewhat different perspectives. I understand that. You know, Donald was very fortunate in his life, and that   \n",
       "3  Well, for one thing - - and before we start on that - - my father gave me a very small loan in 1975, and I built it into a company that's worth many, many billions of dollars, with some of the greatest assets in the world, and I say that only because that's the kind of thinking that our country needs. Our country's in deep trouble. We do n't know what we're doing when it comes to devaluations and all of these countries all over the world, especially China. They're the best, the best ever at it. What they're doing to us is a very, very sad thing. So we have to do that. We have to renegotiate our trade deals. And, Lester, they're taking our jobs, they're giving incentives, they're doing things that, frankly, we do n't do. Let me give you the example of Mexico. They have a VAT tax. We're on a different system. When we sell into Mexico, there                                                                                 \n",
       "4  Secretary Clinton and others, politicians, should have been doing this for years, not right now, because of the fact that we've created a movement. They should have been doing this for years. What's happened to our jobs and our country and our economy generally is - - look, we owe $20 trillion. We can not do it any longer, Lester.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   context/1  \\\n",
       "0  Well, thank you, Lester, and thanks to Hofstra for hosting us. The central question in this election is really what kind of country we want to be and what kind of future we'll build together. Today is my granddaughter's second birthday, so I think about this a lot. First, we have to build an economy that works for everyone, not just those at the top. That means we need new jobs, good jobs, with rising incomes. I want us to invest in you. I want us to invest in your future. That means jobs in infrastructure, in advanced manufacturing, innovation and technology, clean, renewable energy, and small business, because most of the new jobs will come from small business. We also have to make the economy fairer. That starts with raising the national minimum wage and also guarantee, finally, equal pay for women's work.                                                                                                        \n",
       "1  I also want to see more companies do profit - sharing. If you help create the profits, you should be able to share in them, not just the executives at the top. And I want us to do more to support people who are struggling to balance family and work. I've heard from so many of you about the difficult choices you face and the stresses that you're under. So let's have paid family leave, earned sick days. Let's be sure we have affordable child care and debt - free college. How are we going to do it? We're going to do it by having the wealthy pay their fair share and close the corporate loopholes. Finally, we tonight are on the stage together, Donald Trump and I. Donald, it's good to be with you. We're going to have a debate where we are talking about the important issues facing our country. You have to judge us, who can shoulder the immense, awesome responsibilities of the presidency, who can put into action the   \n",
       "2  Secretary Clinton, would you like to respond?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "3  Let me follow up with Mr. Trump, if you can. You've talked about creating 25 million jobs, and you've promised to bring back millions of jobs for Americans. How are you going to bring back the industries that have left this country for cheaper labor overseas? How, specifically, are you going to tell American manufacturers that you have to come back?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "4  Let me interrupt just a moment, but...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          context/2  \\\n",
       "0  Well, I do n't expect us to cover all the issues of this campaign tonight, but I remind everyone, there are two more presidential debates scheduled. We are going to focus on many of the issues that voters tell us are most important, and we're going to press for specifics. I am honored to have this role, but this evening belongs to the candidates and, just as important, to the American people. Candidates, we look forward to hearing you articulate your policies and your positions, as well as your visions and your values. So, let's begin. We're calling this opening segment \"Achieving Prosperity.\" And central to that is jobs. There are two economic realities in America today. There's been a record six straight years of job growth, and new census numbers show incomes have increased at a record rate after years of stagnation. However, income inequality remains significant, and nearly half of Americans are living paycheck to paycheck. Beginning with you, Secretary Clinton, why are you a better choice   \n",
       "1  Well, thank you, Lester, and thanks to Hofstra for hosting us. The central question in this election is really what kind of country we want to be and what kind of future we'll build together. Today is my granddaughter's second birthday, so I think about this a lot. First, we have to build an economy that works for everyone, not just those at the top. That means we need new jobs, good jobs, with rising incomes. I want us to invest in you. I want us to invest in your future. That means jobs in infrastructure, in advanced manufacturing, innovation and technology, clean, renewable energy, and small business, because most of the new jobs will come from small business. We also have to make the economy fairer. That starts with raising the national minimum wage and also guarantee, finally, equal pay for women's work.                                                                                                                                                                                               \n",
       "2  We can not let it happen. Under my plan, I'll be reducing taxes tremendously, from 35 percent to 15 percent for companies, small and big businesses. That's going to be a job creator like we have n't seen since Ronald Reagan. It's going to be a beautiful thing to watch. Companies will come. They will build. They will expand. New companies will start. And I look very, very much forward to doing it. We have to renegotiate our trade deals, and we have to stop these countries from stealing our companies and our jobs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "3  Well, I think that trade is an important issue. Of course, we are 5 percent of the world's population; we have to trade with the other 95 percent. And we need to have smart, fair trade deals. We also, though, need to have a tax system that rewards work and not just financial transactions. And the kind of plan that Donald has put forth would be trickle - down economics all over again. In fact, it would be the most extreme version, the biggest tax cuts for the top percent of the people in this country than we've ever had. I call it trumped - up trickle - down, because that's exactly what it would be. That is not how we grow the economy. We just have a different view about what's best for growing the economy, how we make investments that will actually produce jobs and rising incomes. I think we come at it from somewhat different perspectives. I understand that. You know, Donald was very fortunate in his life, and that                                                                                   \n",
       "4  Well, for one thing - - and before we start on that - - my father gave me a very small loan in 1975, and I built it into a company that's worth many, many billions of dollars, with some of the greatest assets in the world, and I say that only because that's the kind of thinking that our country needs. Our country's in deep trouble. We do n't know what we're doing when it comes to devaluations and all of these countries all over the world, especially China. They're the best, the best ever at it. What they're doing to us is a very, very sad thing. So we have to do that. We have to renegotiate our trade deals. And, Lester, they're taking our jobs, they're giving incentives, they're doing things that, frankly, we do n't do. Let me give you the example of Mexico. They have a VAT tax. We're on a different system. When we sell into Mexico, there                                                                                                                                                                 \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          context/3  \n",
       "0  Good luck to you.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "1  Well, I do n't expect us to cover all the issues of this campaign tonight, but I remind everyone, there are two more presidential debates scheduled. We are going to focus on many of the issues that voters tell us are most important, and we're going to press for specifics. I am honored to have this role, but this evening belongs to the candidates and, just as important, to the American people. Candidates, we look forward to hearing you articulate your policies and your positions, as well as your visions and your values. So, let's begin. We're calling this opening segment \"Achieving Prosperity.\" And central to that is jobs. There are two economic realities in America today. There's been a record six straight years of job growth, and new census numbers show incomes have increased at a record rate after years of stagnation. However, income inequality remains significant, and nearly half of Americans are living paycheck to paycheck. Beginning with you, Secretary Clinton, why are you a better choice  \n",
       "2  Thank you, Lester. Our jobs are fleeing the country. They're going to Mexico. They're going to many other countries. You look at what China is doing to our country in terms of making our product. They're devaluing their currency, and there's nobody in our government to fight them. And we have a very good fight. And we have a winning fight. Because they're using our country as a piggy bank to rebuild China, and many other countries are doing the same thing. So we're losing our good jobs, so many of them. When you look at what's happening in Mexico, a friend of mine who builds plants said it's the eighth wonder of the world. They're building some of the biggest plants anywhere in the world, some of the most sophisticated, some of the best plants. With the United States, as he said, not so much. So Ford is leaving. You see that, their small car division leaving. Thousands of jobs leaving Michigan, leaving Ohio                                                                                          \n",
       "3  Secretary Clinton, would you like to respond?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "4  Let me follow up with Mr. Trump, if you can. You've talked about creating 25 million jobs, and you've promised to bring back millions of jobs for Americans. How are you going to bring back the industries that have left this country for cheaper labor overseas? How, specifically, are you going to tell American manufacturers that you have to come back?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat(list_of_df, ignore_index=True, axis=0).dropna()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's split data into train and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trump_said</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "      <th>context/2</th>\n",
       "      <th>context/3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>No, I do n't know that.</td>\n",
       "      <td>You do n't know that? Okay.</td>\n",
       "      <td>I have no idea. I know nothing about them.</td>\n",
       "      <td>But there's not a Satanic pedophile cult being run by -</td>\n",
       "      <td>... and I agree with it very strongly.</td>\n",
       "      <td>Okay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>But why was he given tens of millions of dollars?</td>\n",
       "      <td>My son like a lot of people at home had a drug problem. He's overtaking it. He's fixed it. He's worked on it. And I 'm proud of him, I' m proud of my son.</td>\n",
       "      <td>He made a fortune and he did n't have a job.</td>\n",
       "      <td>That is not true.</td>\n",
       "      <td>Once you became vice president he made a fortune in Ukraine, in China, in Moscow and various other places.</td>\n",
       "      <td>None of that is true.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>That's a big statement.</td>\n",
       "      <td>Here's the deal -</td>\n",
       "      <td>Oh, I see. Okay.</td>\n",
       "      <td>Because the oil industry pollutes, significantly.</td>\n",
       "      <td>Why would you do that?</td>\n",
       "      <td>Because I would stop.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>I'll fire them.</td>\n",
       "      <td>Well, I'll give you the list of the people who -</td>\n",
       "      <td>I'd like to know who they are.</td>\n",
       "      <td>I did it honorably.</td>\n",
       "      <td>Oh, really?</td>\n",
       "      <td>... testified under oath in his administration said I did my job and I did it very well.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>It's nice to - - one on three.</td>\n",
       "      <td>Ken Karpowicz has a question.</td>\n",
       "      <td>No, it has n't. It has n't. And it has n't been finished at all.</td>\n",
       "      <td>We brought up the e - mails.</td>\n",
       "      <td>I'd like to know, Anderson, why are n't you bringing up the e - mails? I'd like to know. Why are n't you bringing...</td>\n",
       "      <td>We have a question here from Ken Karpowicz. He has a question about health care. Ken?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Trump_said  \\\n",
       "789  No, I do n't know that.                             \n",
       "478  But why was he given tens of millions of dollars?   \n",
       "715  That's a big statement.                             \n",
       "404  I'll fire them.                                     \n",
       "159  It's nice to - - one on three.                      \n",
       "\n",
       "                                                                                                                                                        context  \\\n",
       "789  You do n't know that? Okay.                                                                                                                                  \n",
       "478  My son like a lot of people at home had a drug problem. He's overtaking it. He's fixed it. He's worked on it. And I 'm proud of him, I' m proud of my son.   \n",
       "715  Here's the deal -                                                                                                                                            \n",
       "404  Well, I'll give you the list of the people who -                                                                                                             \n",
       "159  Ken Karpowicz has a question.                                                                                                                                \n",
       "\n",
       "                                                            context/0  \\\n",
       "789  I have no idea. I know nothing about them.                         \n",
       "478  He made a fortune and he did n't have a job.                       \n",
       "715  Oh, I see. Okay.                                                   \n",
       "404  I'd like to know who they are.                                     \n",
       "159  No, it has n't. It has n't. And it has n't been finished at all.   \n",
       "\n",
       "                                                   context/1  \\\n",
       "789  But there's not a Satanic pedophile cult being run by -   \n",
       "478  That is not true.                                         \n",
       "715  Because the oil industry pollutes, significantly.         \n",
       "404  I did it honorably.                                       \n",
       "159  We brought up the e - mails.                              \n",
       "\n",
       "                                                                                                                context/2  \\\n",
       "789  ... and I agree with it very strongly.                                                                                 \n",
       "478  Once you became vice president he made a fortune in Ukraine, in China, in Moscow and various other places.             \n",
       "715  Why would you do that?                                                                                                 \n",
       "404  Oh, really?                                                                                                            \n",
       "159  I'd like to know, Anderson, why are n't you bringing up the e - mails? I'd like to know. Why are n't you bringing...   \n",
       "\n",
       "                                                                                    context/3  \n",
       "789  Okay.                                                                                     \n",
       "478  None of that is true.                                                                     \n",
       "715  Because I would stop.                                                                     \n",
       "404  ... testified under oath in his administration said I did my job and I did it very well.  \n",
       "159  We have a question here from Ken Karpowicz. He has a question about health care. Ken?     "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_df, val_df = train_test_split(df, test_size = 0.1)\n",
    "trn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer, eos = True):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
    "    conv = flatten(conv)\n",
    "    return conv\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
    "\n",
    "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
    "\n",
    "        directory = args.cache_dir\n",
    "        cached_features_file = os.path.join(\n",
    "            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n",
    "        )\n",
    "\n",
    "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"rb\") as handle:\n",
    "                self.examples = pickle.load(handle)\n",
    "        else:\n",
    "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "\n",
    "            self.examples = []\n",
    "            for _, row in df.iterrows():\n",
    "                conv = construct_conv(row, tokenizer)\n",
    "                self.examples.append(conv)\n",
    "\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"wb\") as handle:\n",
    "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Cacheing and storing of data/checkpoints\n",
    "\n",
    "def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n",
    "    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n",
    "    ordering_and_checkpoint_path = []\n",
    "\n",
    "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n",
    "\n",
    "    for path in glob_checkpoints:\n",
    "        if use_mtime:\n",
    "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
    "        else:\n",
    "            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
    "            if regex_match and regex_match.groups():\n",
    "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
    "\n",
    "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
    "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
    "    return checkpoints_sorted\n",
    "\n",
    "\n",
    "def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n",
    "    if not args.save_total_limit:\n",
    "        return\n",
    "    if args.save_total_limit <= 0:\n",
    "        return\n",
    "\n",
    "    # Check if we should delete older checkpoint(s)\n",
    "    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n",
    "    if len(checkpoints_sorted) <= args.save_total_limit:\n",
    "        return\n",
    "\n",
    "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
    "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "    for checkpoint in checkpoints_to_be_deleted:\n",
    "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
    "        shutil.rmtree(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating\n",
    "Now that we have THE DATA we can finally create our model and start training it! The training and evaluation loop are quite simple. We simplely take a batch of examples from our dataloader and use it both as our inputs and labels. We do this because GPT2 is an auto-regressive model, meaning it uses some context to predict the next token. This prediction is then added to the original context and fed back in as the new context for generating the next token.\n",
    "\n",
    "To evaluate our model, we use the metric perplexity, which is a simple, but powerful metric. Perplexity is a measure of how unsure the model is in its choice of the next token. The more unsure our model is, the higher its perplexity. One fascinating thing about perplexity is that it correlates very well with what humans think of when it comes to coherent and specific natural conversations, which was shown in the amazing paper [\"Towards a Human-like Open-Domain Chatbot\"](https://arxiv.org/abs/2001.09977) by Daniel Adiwardana, et. al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    # add_special_tokens_(model, tokenizer)\n",
    "\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if (\n",
    "        args.model_name_or_path\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
    "    ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
    "        )\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
    "        try:\n",
    "            # set global_step to gobal_step of last saved checkpoint from model path\n",
    "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
    "            global_step = int(checkpoint_suffix)\n",
    "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "\n",
    "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "        except ValueError:\n",
    "            logger.info(\"  Starting fine-tuning.\")\n",
    "\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    # set_seed(args)  # Added here for reproducibility\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            inputs, labels = (batch, batch)\n",
    "            if inputs.shape[1] > 1024: continue\n",
    "            inputs = inputs.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            model.train()\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    if (\n",
    "                        args.local_rank == -1 and args.evaluate_during_training\n",
    "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results = evaluate(args, model, tokenizer)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    checkpoint_prefix = \"checkpoint\"\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    model_to_save = (\n",
    "                        model.module if hasattr(model, \"module\") else model\n",
    "                    )  # Take care of distributed/parallel training\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
    "\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step\n",
    "\n",
    "# Evaluation of some model\n",
    "\n",
    "def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n",
    "    os.makedirs(eval_output_dir, exist_ok=True)\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        inputs, labels = (batch, batch)\n",
    "        inputs = inputs.to(args.device)\n",
    "        labels = labels.to(args.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            lm_loss = outputs[0]\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    result = {\"perplexity\": perplexity}\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Main runner\n",
    "\n",
    "def main(df_trn, df_val):\n",
    "    args = Args()\n",
    "    \n",
    "    if args.should_continue:\n",
    "        sorted_checkpoints = _sorted_checkpoints(args)\n",
    "        if len(sorted_checkpoints) == 0:\n",
    "            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
    "        else:\n",
    "            args.model_name_or_path = sorted_checkpoints[-1]\n",
    "\n",
    "    if (\n",
    "        os.path.exists(args.output_dir)\n",
    "        and os.listdir(args.output_dir)\n",
    "        and args.do_train\n",
    "        and not args.overwrite_output_dir\n",
    "        and not args.should_continue\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "                args.output_dir\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        args.local_rank,\n",
    "        device,\n",
    "        args.n_gpu,\n",
    "        bool(args.local_rank != -1),\n",
    "        args.fp16,\n",
    "    )\n",
    "\n",
    "    # Set seed\n",
    "    # set_seed(args)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
    "    model = AutoModelWithLMHead.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=False,\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "    model.to(args.device)\n",
    "    \n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "    # Training\n",
    "    if args.do_train:\n",
    "        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n",
    "\n",
    "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
    "    if args.do_train:\n",
    "        # Create output directory if needed\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        model_to_save = (\n",
    "            model.module if hasattr(model, \"module\") else model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "        # Load a trained model and vocabulary that you have fine-tuned\n",
    "        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "        model.to(args.device)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if args.do_eval and args.local_rank in [-1, 0]:\n",
    "        checkpoints = [args.output_dir]\n",
    "        if args.eval_all_checkpoints:\n",
    "            checkpoints = list(\n",
    "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "            )\n",
    "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "        for checkpoint in checkpoints:\n",
    "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
    "\n",
    "            model = AutoModelWithLMHead.from_pretrained(checkpoint)\n",
    "            model.to(args.device)\n",
    "            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n",
    "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
    "            results.update(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/26/2020 10:58:58 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "/Users/viktor/opt/anaconda3/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:890: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "12/26/2020 10:59:08 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x7fb81da656a0>\n",
      "12/26/2020 10:59:08 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "12/26/2020 10:59:09 - INFO - __main__ -   Saving features into cached file cached/gpt2_cached_lm_512\n",
      "12/26/2020 10:59:09 - INFO - __main__ -   ***** Running training *****\n",
      "12/26/2020 10:59:09 - INFO - __main__ -     Num examples = 830\n",
      "12/26/2020 10:59:09 - INFO - __main__ -     Num Epochs = 3\n",
      "12/26/2020 10:59:09 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "12/26/2020 10:59:09 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "12/26/2020 10:59:09 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "12/26/2020 10:59:09 - INFO - __main__ -     Total optimization steps = 621\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412eb7022d404a2790830f04abb03b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch'), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f684a7d281a043f9980e6862ee5e09ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Iteration'), FloatProgress(value=0.0, max=207.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ec4734fad44968a418fbf58ab96ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Iteration'), FloatProgress(value=0.0, max=207.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252a3b383681489ba4ba9825b80fd26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Iteration'), FloatProgress(value=0.0, max=207.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/26/2020 15:47:47 - INFO - __main__ -    global_step = 621, average loss = 1.9892467788260149\n",
      "12/26/2020 15:47:47 - INFO - __main__ -   Saving model checkpoint to output\n",
      "12/26/2020 15:47:53 - INFO - __main__ -   Evaluate the following checkpoints: ['output']\n",
      "12/26/2020 15:47:56 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "12/26/2020 15:47:56 - INFO - __main__ -   Saving features into cached file cached/gpt2_cached_lm_512\n",
      "12/26/2020 15:47:56 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/26/2020 15:47:56 - INFO - __main__ -     Num examples = 93\n",
      "12/26/2020 15:47:56 - INFO - __main__ -     Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803772282f1945cc8194a93d1f8f0bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Evaluating'), FloatProgress(value=0.0, max=23.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/26/2020 15:50:32 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/26/2020 15:50:32 - INFO - __main__ -     perplexity = tensor(4.9978)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity_': tensor(4.9978)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(trn_df, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatting with our Model\n",
    "\n",
    "Now that we have our model trained, let's it out for a spin and have our first conversation with Donald Trump!\n",
    "\n",
    "The below code is copied pretty much verbatim from the creators of the DialoGPT model, which you can find [here](https://huggingface.co/microsoft/DialoGPT-small).\n",
    "\n",
    "Moreover, let's compare the result with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: How are you, Mr. Trump?\n",
      "Trump: I'm not.\n",
      ">> User: Are you planning to start a nuclear war with North Korea?\n",
      "Trump: Okay.\n",
      ">> User: Are you planning to continue the economic war with China?\n",
      "Trump: !!!I' m not going to continue that economic war.\n",
      ">> User: Have you been supported by Russia during the election?\n",
      "Trump: No, I have not been supported, by Russia.\n",
      ">> User: How do you feel about the Black Life Matters?\n",
      "Trump: That's a good question.\n",
      ">> User: How are you planning to overcome Covid-19?\n",
      "Trump: Well, let me ask you this.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
    "model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n",
    "\n",
    "# Let's chat for 6 lines\n",
    "for step in range(6):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User: \") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=300,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=100, \n",
    "        top_p=0.7,\n",
    "        temperature = 0.8\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"Trump: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:Good evening!\n",
      "Trump: It's been a very tough week.\n",
      ">> User:Are you plan to start the war with North Korea\n",
      "Trump: !!!The war with China is going to end in a matter of weeks.\n",
      ">> User:Are you planning to continue the economic war with China?\n",
      "Trump: !The economic war is going on with China.\n",
      ">> User:Have you been supported by Russia during the election?\n",
      "Trump: I have been supporting China. And you know what? I have been supported for the last two years, and I have supported Russia, for a long time.\n",
      ">> User:How do you feel about the Black Life Matters?\n",
      "Trump: And I have to say, you have to be a leader.\n",
      ">> User:How are you planning to overcome Covid-19?\n",
      "Trump: All right, gentlemen.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n",
    "\n",
    "# Let's chat for 6 lines\n",
    "for step in range(6):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=300,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=100, \n",
    "        top_p=0.7,\n",
    "        temperature = 0.8\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"Trump: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use another model, without normalization of the text and limitation for tokens. perplexity = tensor(4.5344)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: Good evening!\n",
      "Trump: I want to say it again.\n",
      ">> User: Are you planning to start a nuclear war with North Korea?\n",
      "Trump: Are they planning to launch a nuclear strike on the United States?\n",
      ">> User: Are you planning to continue the economic war with China?\n",
      "Trump: !!!Wrong!!\n",
      ">> User: Have you been supported by Russia during the election?\n",
      "Trump: What are you going to do to stop the economic depression that's coming in from China?!!You're going to have a big economy.!!We’re going to be in a big depression.!We are going to make China pay for it.!\n",
      ">> User: How do you feel about the Black Life Matters?\n",
      "Trump: That’s not what it’ll be.\n",
      ">> User: How are you planning to overcome Covid-19?\n",
      "Trump: I’m not going to give you a chance.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
    "model = AutoModelWithLMHead.from_pretrained(\"output_full_trump\")\n",
    "\n",
    "# Let's chat for 6 lines\n",
    "for step in range(6):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User: \") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=300,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=100, \n",
    "        top_p=0.7,\n",
    "        temperature = 0.8\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"Trump: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good evening!\n",
    "# Are you planning to start a nuclear war with North Korea?\n",
    "# Are you planning to continue the economic war with China?\n",
    "# Have you been supported by Russia during the election?\n",
    "# How do you feel about the Black Life Matters?\n",
    "# How are you planning to overcome Covid-19?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use another model, with normalization the text + lowercase and limitation for tokens = 100. perplexity = tensor(6.5417)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: Good evening!\n",
      "Trump: good evening.\n",
      ">> User: Are you planning to start a nuclear war with North Korea?\n",
      "Trump: i think i have to ask you a question.\n",
      ">> User: Are you planning to continue the economic war with China?\n",
      "Trump: it's a good question. we're going to continue to be in a very good position. we are going to be on the very expensive side. we have to be able to provide for the people who depend on us. we will be able, but we will have to provide some very good support. we can be on our side.\n",
      ">> User: Have you been supported by Russia during the election?\n",
      "Trump: yes.\n",
      ">> User: How do you feel about the Black Life Matters?\n",
      "Trump: !!!excuse me, sir.\n",
      ">> User:  How are you planning to overcome Covid-19?\n",
      "Trump: excuse.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
    "model = AutoModelWithLMHead.from_pretrained(\"output_100_lower\")\n",
    "\n",
    "# Let's chat for 6 lines\n",
    "for step in range(6):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User: \") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=100, \n",
    "        top_p=0.7,\n",
    "        temperature = 0.8\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"Trump: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good evening!\n",
    "# Are you planning to start a nuclear war with North Korea?\n",
    "# Are you planning to continue the economic war with China?\n",
    "# Have you been supported by Russia during the election?\n",
    "# How do you feel about the Black Life Matters?\n",
    "# How are you planning to overcome Covid-19?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it ain't the best, however, training it for longer or using the DialoGPT-medium instead of DialoGPT-small does improve results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "\n",
    "The models are not perfect. How can we increase quality?\n",
    "* improve the dataset, fix bugs, etc. Make the text, speech smaller, for example, generate a summary for a text, remove noise sentence (leave only important, questions).\n",
    "* increase amout of data: tweets, trump speech and wikipedia + question-genereted model https://github.com/patil-suraj/question_generation. Example: Addtional_dataset.ipynb.\n",
    "* use bigger models DialoGPT-medium https://huggingface.co/microsoft/DialoGPT-medium or DialoGPT-large https://huggingface.co/microsoft/DialoGPT-large\n",
    "* have longer training and play around with parameters of training.\n",
    "* Using a bot to generate the conversion and after that using it in the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
